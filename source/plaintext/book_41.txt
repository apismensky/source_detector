Route Reconstruction system we saw in Chapter 3 might expand on these amounts of relevant data a little by creating ‘sausages’ of data around an image, constructing the history and future around a specifc image selected by the algorithms. Still the technology ought to be able to select out huge amounts of irrelevant data for deletion. Even data that appeared to be initially relevant and was shown on the User Interface to operatives of the surveillance system and Route Reconstruction data would only be kept for a short time until reviewed by operatives who could also declare the images irrelevant and send them for deletion. At the end of the experimental phase of the project, it might seem farfetched to describe deletion as a disorderly and disruptive blank fgure based on complex qualculations of quantities and qualities. Relevant data could be checked and then deleted. Irrelevant data, by default, would be all the other data. This apparent certainty, at least at this stage of the project, extended through the algorithmic system. The IF-THEN rules 84 D. NEYLAND were clear, the maps of the fxed attributes of the experimental settings were clear, the models for object classifcation and the action states of objects as worthy of further scrutiny all seemed clear. The quantities involved were signifcant—terabytes of digital video data—but the qualities—mostly operatives clicking on text alerts and watching short videos, were neatly contained. Following Callon and Law (2005), we could say that this was the frst step towards a straightforward form of qualculation. Things were separated out and disentangled such that they might be recombined in a single space (within the algorithmic system). The background subtraction technique that we saw in Chapter 2 provided this seemingly straightforward basis for beginning demarcations of relevant data (to be kept) and irrelevant data (to be deleted). A result could be extracted. However, the project was now moving beyond its initial experimental phase. In the airport and train station as the technology moved towards system testing, the computer scientists from University 1 and 2 began to engage with the complexities of relevance detection in real time and real space. They started to look for ways to tidy up the initial steps of object classifcations (which provided approximate shapes for background subtraction) in the airport and train station, through ever more closely cropped pixel masks for objects, with any single, isolated pixels erased and any holes between pixels flled. They suggested masks could be further tidied by removing shadow, just leaving the new entity. And these tidied up entities could now be subjected to object classifcation with what the computer scientists hoped was greater certainty. They were cleaned and tidied objects. Object classifcation would now defne with confdence the objects in view as, for example, human-shaped or luggage-shaped. Cleaning the images, removing shadow, removing gaps in pixel masks was more processor intensive than the initial quick and dirty technique we noted in the earlier experimental phase of the project, but it was still computationally elegant for the computer scientists. It was a reasonably quick technique for ascertaining a classifcation of putative objects and it was a classifcation in which they (and other project participants) could have confdence. Object classifcation required this more developed form of qualculation, drawing entities together into new relations such that they might be qualifed for judging as relevant or irrelevant because the system faced new challenges in working in real spaces in real time. Classifying 4 THE DELETING MACHINE AND ITS DISCONTENTS 85 something as a human-shaped object in object classifcation still involved algorithmic analysis of video streams in order to draw the parameters (size and shape) of human-shaped or other shaped objects, it still required background subtraction and each object was still identifed through a vector of around 200 features, so each object in itself was complicated. But the airport and train station involved far more cameras than initial experimentation, data in a wider array of formats and framerates, a far greater number of human-shaped and other objects. Confdence in the system’s ability to demarcate relevant from irrelevant data had to remain high as the algorithmic system required further development in order to work in the airport and train station. In particular, object tracking in the airport and train station needed to be attuned to the specifcities of the spaces in which it would work. Object tracking just like our abandoned luggage algorithm had to be able to grasp everyday life. Object tracking was vital for the Route Reconstruction system to work and follow a human-shaped object across multiple cameras, and for the system to know if human-shaped and luggage-shaped objects were moving apart, to know if human-shaped objects were moving the wrong way 