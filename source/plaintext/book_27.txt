relevance detection algorithm would work. The Event Detection system would sift through video frames travelling through the system and use the media proxy we met in Chapter 2 to draw together the streams of video from cameras across the surveillance network. This would use a Real-Time Streaming Protocol for MPEG4 using JSON (JavaScript Object Notifcation) as a data interchange format for system analysis. Each stream would have a metadata time stamp. The relevance detection algorithms for abandoned luggage, moving the wrong way and entering a forbidden space would then select out putative object types (using a Codebook algorithm for object detection) focusing on their dimensions, direction and speed. As we noted in Chapter 2, the system would then generate bounding boxes for objects that would then generate a stream of metadata related to the bounding box based 3 ACCOUNTABILITY AND THE ALGORITHM 55 on its dimensions and timing—how fast it moved and in what direction. This would also require a further development of the map of fxed attributes used for background subtraction in Chapter 2. Areas where entry was forbidden for most people (e.g. secure areas and train tracks) and areas where the direction of movement was sensitive (e.g. exits at peak times in busy commuter train stations) would need to be added to the maps. Producing an alert was no longer limited to identifying a human-shaped object (or luggage-shaped or any other shaped object)— even though that was challenging in its own ways. The system would now have to use these putative classifcations to identify those humanshaped objects moving in the wrong direction or into the wrong space, along with those human-shaped objects that became separate from their luggage-shaped objects. Objects’ action states as moving or still, for example, would be central. For the algorithms to be able to do this demonstratively within the airport and train station was crucial to being able to produce alerts and participate in account-ability. But this didn’t reduce the surveillance operatives’ concerns about the form in which they would receive these alerts. Participating in account-ability was not just about producing an alert. The alerts had to accomplish what Garfnkel (1963) termed the congruence of relevances. Garfnkel suggested that any interaction involved successive turns to account-ably and demonstrably make sense of the scene in which the interactions were taking place. This required the establishment of an at least in-principle interchangeability of viewpoints—that one participant in the interaction could note what was relevant for the other participants, could make sense of what was relevant for themselves and the other participants and could assume that other participants shared some similar expectations in return. Relevances would thus become shared or congruent through the interaction. Garfnkel (1963) suggested that these were strongly adhered to, forming what he termed constitutive expectancies for the scene of the interaction. In this way, building a shared sense of the interaction, a congruence of relevances, was constitutive of the sense accomplished by the interaction. The algorithmic system seemed to propose a future that stood in some contrast to the operatives’ current ways of working. Prior to the algorithmic system, surveillance operatives’ everyday competences were oriented towards working with images, other operatives, airport or train station employees, their managers and so on, in making a sense of the scene. The rich and detailed interaction successively built a sense of what 56 D. NEYLAND it was that was going on. Constitutive expectancies seemed to be set in place. The move to limit the amount of data that was seen seemed to reduce the array of image-based cues through which accomplishing the sense of a scene could take place. Given that an ethical aim of the project was to reduce the scope of data made visible and given that this was central to the funding proposal and its success, the computer scientists needed to fnd a way to make this work. They tried to work through with the surveillance operatives how little they needed to see for the system still to be considered functional. In this way, the ethical aims of the project began to form part of the account-able order of the algorithmic system that was emerging in this experimental phase of the project. Decision-making was demonstrably organised so that it could be seen to match the ethical aims of the project at the same time as the emerging system could be constituted as a particular material-algorithmic instantiation of the ethical aims. Accomplishing this required resolution of issues for the operatives and the computer scientists of just what should be seen and how should such visibility be managed. This required a series of decisions to be made about the User Interface. The computer scientists suggested that one way to move forward with the ethical aims of the project was to develop 