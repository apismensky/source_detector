have been mostly focused on online data, these policy moves have also spurred broader concerns with data repositories and data analysis and the posited need for erasure. For example, erasure, forgetting and accountability have become key reference points in the development of what have become termed Privacy Enhancing Technologies (PETs) and Privacy by Design projects (see Goold 2009). Here the remit for data storage and analysis is not restricted to online data but also incorporates concerns with the kinds of video-based data that our algorithms specialise in. The premise of these arguments for PETs is that all algorithmic technologies ought to take privacy concerns into account. In these discussions, privacy is often understood in more or less straightforward binary terms. For example, it is proposed that if one’s data no longer exists, there is no risk to one’s privacy. One type of emerging PET within this feld is autodeletion technologies (also see Mayer-Schonberger 2009). To delete and to accountably demonstrate that deletion has taken place appears to be an emerging benchmark for policy compliance. For the coordinators of the project, being able to set such a benchmark through the emerging system would be a step towards market launch (see Chapter 6). It was not only the ethical aims of the project that were at stake in developing a means to delete but the future market viability of the technology. Regulatory compliance could be sold on the open market. The coordinator’s search for a means to go beyond a conventional approach to deletion which involves simply changing the connections through which a user might access data was part of this preparatory market work. The conventional approach to deleting supported by the computer scientists, was unlikely to fulfl the proposed terms of policy mechanisms such as the revised EU General Data Protection Regulation or the concerns articulated in the literature on PETs and Privacy by Design. The concern articulated as prompting the right to be forgotten/right to erasure is couched in terms of a need to expunge data from 4 THE DELETING MACHINE AND ITS DISCONTENTS 77 a repository, making it impossible to link, scrape, share or make further uses of that data; it is argued that to simply change the route via which information is retrieved can be overcome with little effort and reopens the data to all future uses. And the Article 29 Working Party accountability principle requires that compliance with such expunging is made clearly and demonstrably available. Deletion then sits centrally within the development of our algorithms. To be able to select out images and send them to operatives as alerts was a technical achievement, but to competently delete would require selecting out all data that did not need to be sent to operatives and developing a system for its removal. This is a challenging basis for research for an ethnographic social scientist. The very thing being studied is always and already in the processes of becoming nothing. It is a double negation: data that has been deemed irrelevant is the thing that we need to study in this chapter and data that has been deemed irrelevant needs to be studied because it will be deleted. Studying irrelevance heading towards digital oblivion seems a challenge. In practice both data and deletion can be traced up to a point, but then (at least in theory) it should be gone. How can we grasp this partial and momentary thing—the action of deletion—along with this stuff that is here for a time and then goes—the irrelevant data? One way forward is to return to the detail of the algorithms. If we can make sense of how the algorithms participate in the selection of things that are irrelevant and to be deleted and we can then fgure out how those things are deleted (or as it turns out, not very well deleted), that might be a start. One way to work through the complexities of deletion is to make sense of what it is: a system for turning the complexities and uncertainties of the everyday into a basis for calculating and dividing relevance from irrelevance. As Callon and Muniesa (2005) suggest on calculating: A calculative agency will be all the more powerful when it is able to: a) establish a long, yet fnite list of diverse entities; b) allow rich and varied relations between the entities thus selected, so that the space of possible classifcations and reclassifcations is largely open; c) formalize procedures and algorithms likely to multiply the possible hierarchies and classifcations between these entities. As this calculative power depends on the equipments that agencies can rely upon, we can easily understand why it is unevenly distributed among them. (1238) 78 D. NEYLAND We can think of our algorithms on these terms: they establish a fnite list of entities (human-shaped objects, luggage-shaped objects, bounding boxes and close-cropped images), entered into varied relations (object action states such as moving the wrong way or 