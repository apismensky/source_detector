of the conventional surveillance system suggested that over a 6 hour period, approximately 6 suspicious items that might turn out to be lost or abandoned luggage, would be fagged by the operatives and sent to security operatives on the ground for further scrutiny. On this basis, our abandoned luggage algorithm and its IF-THEN rules (see Introduction and Chapter 2) needed to perform at least to this level for the comparative measure to do its work and demonstrate that the future would be as effective as the present, but with added ethics. The system was set to run for 6 hour prior to the arrival in the offce of the surveillance operatives so they could be given the results of the comparative metric. I had also taken an interest in these comparative metrics. I wanted to know how the effectiveness of our algorithms could be made calculable, what kinds of devices this might involve, how entities like false positives (seeing things that were not there) and false negatives (not seeing things that were there) might be constituted. I wanted to relay these results to the ethical experts who had taken part in the previous demonstrations on the basis that a clear division between technical effcacy and ethical achievement was not possible (see Chapter 3 for more on ethics). If the system worked or did not on this criteria, would provide a further basis for ethical scrutiny. In the 6 hour that the system ran, when the conventional surveillance system would detect 6 items of potentially lost or abandoned luggage, the algorithmic system detected 2654 potentially suspicious items. This result went so far off the scale of predicted events, that the accuracy of the system could not even be measured. That is, there were just too many alerts for anyone to go through and check the number of false positives. The working assumption of the computer scientists was that there were likely to be around 2648 incorrect classifcations of human-shaped and luggage-shaped objects that had for a time stayed together and then separated. In later checking of a random sample of alerts, it turned out the system was detecting as abandoned luggage such things as refective surfaces, sections of wall, a couple embracing and a person studying a departure board. Some of these were not fxed attributes of the airport and so did not feature in the digital maps that were used for background subtraction. However, object parameterisation should have been able to calculate that these were not luggage-shaped objects, and the fooring and walls should have been considered fxed attributes. 104 D. NEYLAND However, in the immediate situation of the demonstration, there was not even time for this random sampling and its hastily developed explanations—these all came later. The airport surveillance operatives turned up just as the 2654 results were gathered together and the project team had to meekly hand these results to the operatives’ manager as evidence of system (in)effcacy. The results of these tests also highlighted the limitations of my initial ethical demonstrations (described previously). The ‘recorded’ footage of the system in operation that I had (apparently) simply replayed to audiences, began to seem distinctly at odds with the results from the live testing. What was the nature of the videos that I had been showing in these demonstrations? On further discussion with the computer scientists in the project, it turned out that system accuracy could be managed to the extent that the parameters of the footage feeding into the system could be controlled. For example, the computer scientists had worked out that a frame rate of 15 frames per second was ideal for providing enough detail without overloading the system with irrelevant footage. This frame rate enabled the system to work elegantly (see Chapter 2); using just enough processing power to produce results, in real time. They also suggested that certain types of camera location (particularly those with a reasonably high camera angle, no shiny foors and consistent lighting) led to better results for the system. And the conditions of flming were also a pertinent matter; crowds of people, sunshine and too much luggage might confuse the system. As we can see in the following images (Figs. 5.1, 5.2, and 5.3), the system often became ‘confounded’ (to use a term from the computer scientists). Collins (1988) and Coopmans (2010) suggest that central to demonstrations are questions of who is in a position to see what. However, the demonstrations considered here suggest that seeing is not straightforwardly a matter of what is revealed and what is concealed. In the development and demonstration of the algorithmic system, the straightforward division is made more complex between the seeing demonstrator and the audience whose vision is heavily managed. As a researcher and demonstrator, I was continually developing my vision of the algorithms and, in different ways, the end-users as audience were presented with stark 