AND THE ALGORITHM 63 From a social scientist perspective it is not enough to have just an abstract account for ethical consideration. A closer understanding can be brought about by [my presentation’s] further insight into how [the system] will work. The way the system ‘will work’—its means of making sense of the space of the airport and train station—encouraged a number of questions from the ethics board, enabling the system to be held accountable. For example, the Data Protection Offcers involved in the board asked during the frst meeting: Is there a lot of prior data needed for this system? More so than before? Are people profled within the system? How long will the system hold someone’s features as identifable to them as a tagged suspect? These questions drew attention to matters of concern that could be taken back to the project team and publicly reported (in the minutes of the ethics board) and subsequently formed the basis for response and further discussion at the second ethics board meeting. The questions could provide a set of terms for making the algorithmic system accountable through being made available (in public reports) for broader consideration. The questions could also be made part of the account-able order of the algorithmic system, with design decisions taken on the basis of questions raised. In this way, the computer scientists could ensure that there was no mechanism for loading prior data into the system (such as a person’s dimensions, which might lead to them being tracked), and to ensure that metadata (such as the dimensions of human-shaped objects) were deleted along with video data to stop individual profles being created or to stop ‘suspects’ from being tagged. Data Protection Offcers sought to ‘use the committee meetings to clearly shape the project to these serious considerations.’ The ‘serious considerations’ here were the ethical aims. One of the representatives of the civil liberties groups also sought to utilise the access offered by the ethics board meetings but in a different way, noting that ‘As systems become more invisible it becomes more diffcult to fnd legitimate forms of resistance.’ To ‘shape the project’ and ‘fnd legitimate forms of resistance’ through the project seemed to confrm the utility of intersecting account-ability and accountability, opening up distinct ways for the 64 D. NEYLAND system to be questioned and for that questioning to be communicated to further interested audiences. However, as the project progressed, a series of issues emerged that complicated my presentation of the account-able order of the algorithmic system to the ethics board and hence made the intersection of account-ability and accountability more diffcult. For example, I reported to the ethics board a series of issues involved in system development. This included a presentation of the challenges involved in ‘dropping in’ existing algorithms. Although one of the project’s opening ethical aims was that no new algorithms would be developed and that existing algorithms could be ‘dropped into’ existing surveillance networks, these were also termed ‘learning’ algorithms. I presented to the ethics board an acknowledgement from both teams of computer scientists that the algorithms needed to ‘learn’ to operate in the end-user settings; that algorithms for relevancy detection and the Route Reconstruction component had to run through streams of video data; that problems in detecting objects and movements had to be continually reviewed; and that this took ‘10s of hours.’ When problems arose in relation to the lighting in some areas of end-user sites (where, e.g., the glare from shiny airport foors appeared to baffe our abandoned luggage algorithm which kept constituting the glare as abandoned luggage), the code/software tied to the relevancy detection algorithm had to be developed—this I suggested to the ethics board is what constituted ‘learning.’ These ongoing changes to the system through ‘learning’ emphasised the complexities of making sense of the algorithmic system’s account-able order; the way the system went about making sense changed frequently at times as it was experimented with and my reporting to the ethics board needed to manage and incorporate these changes. Alongside the continual development of ‘learning’ algorithms, other issues that emerged as the system developed included an initial phase of experimentation where none of the system components would interact. In this instance, it turned out that one of the project members was using obsolete protocols (based on VAPIX), which other project members could not use or did not want to use. Attempting to resolve this issue took 114 e-mails and four lengthy telephone conference calls in one month of the project. Other issues that emerged included: questions of data quality, frame rates, trade union concerns, pixilation and compression of video streams, which each led to changes in the ways in which the system would work. In 