processing power required to sift through all the data produced in a train station or airport in real time and classify all human-shaped objects precisely would be signifcant. Face recognition, iris recognition and gait recognition (based on how people walked) were all ruled out as insuffciently concise. These approaches may have been persuasive as a means to identify specifc individuals in particular spaces, but their reliability depended on having people stand still in controlled spaces and have their features read by the system for a few seconds. This would not be very satisfying for passengers or airports whose business models depended on the rapid movement of passengers towards shops (Woolgar and Neyland 2013). How then to be concise and satisfying and persuasive in classifying human-shaped objects? As Bowker and Star (2000) suggest, classifcation systems are always incomplete. This incompleteness ensures an ambiguity between the focus of algorithmic classifcation (the putative humanshaped object) and the entity being classifed (the possible human going about their business). Concision requires various efforts to classify to a degree that is satisfying and persuasive in relation to the needs of the system and the audiences for the algorithmic system. The system needs to do enough (be satisfying and persuasive), but no more than enough (be concise) as doing more than enough would require more processing of data. In the process of experimenting with human-shaped objects 2 EXPERIMENTATION WITH A PROBABLE HUMAN-SHAPED OBJECT 35 in this project, various more or less concise ways to classify were drawn up and considered or abandoned either because they would require too much processing power (probably quite persuasive but not concise) or were too inaccurate (quite concise, but produced results that were not at all persuasive). At particular moments, (not very serious) consideration was even given to change the everyday life into which the algorithms would enter in order to make classifcation a more straightforward matter. For example, computer scientists joked about changing the airport architecture to suit the system, including putting in higher ceilings, consistent lighting and fooring, and narrow spaces to channel the fow of people. These were a joke in the sense that they could never be accommodated within the project budget. Elegance had practical and fnancial constraints. A frst move in classifying objects was to utilise a standard practice in video analytics: background subtraction. This method for identifying moving objects was somewhat time-consuming and processor intensive, and so not particularly elegant. But these efforts could be ‘front-loaded’ prior to any active work completed by the system. ‘Front-loading’ in this instance meant that a great deal of work would be done to produce an extensive map of the fxed attributes of the setting (airport or train station) prior to attempts at classifcation work. Mapping the fxed attributes would not then need to be repeated unless changes were made to the setting (such changes included in this project a change to a shopfront and a change to the layout of the airport security entry point). Producing the map provided a basis to inform the algorithmic system what to ignore, helping to demarcate relevance and irrelevance in an initial manner. Fixed attributes were thus nominally collated as non-suspicious and irrelevant in ways that people and luggage, for example, could not be, as these latter objects could not become part of the map of attributes (the maps were produced based on an empty airport and train station). Having a fxed map then formed the background from which other entities could be noted. Any thing that the system detected that was not part of the map would be given an initial putative identity as requiring further classifcation. The basis for demarcating potentially relevant objects depended to some degree, then, on computer scientists and their understanding of spaces such as airports, maps that might be programmed to ignore for a time certain classes of objects as fxed attributes, and classifcation systems that might then also—if successful—provide a hesitant basis for 36 D. NEYLAND selecting out potentially relevant objects. It is clear that anything that algorithms were able to ‘do’ was situated in continual reconfgurations of the entities involved—making sense of the everyday life of the algorithm was thus central. Mapping for background subtraction was only a starting point. Objects noted as non-background entities then needed to be further classifed. Although background subtraction was a standard technique in video analytics, further and more precise object classifcation was the subject of some discussion in the project. Eventually, two means were set for classifying those entities noted as distinct from the background that might turn out to be human-shaped objects, and these became the focus for more intense experimentation in the 